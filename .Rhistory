max(sample(c(0:99), 100)/(99))
assortativity(net,factor(aType))
# Chunk 1: setup
knitr::opts_chunk$set(echo = T, fig.align = "center", out.width = '80%', warning = F, message = F)
# Chunk 3
load("Datasets/russian_trolls.RData")
library(igraph)
# Chunk 4
net = graph_from_edgelist(Y,directed = F)
for(j in 1:ncol(X)){
net = net %>%
set_vertex_attr(name = colnames(X)[j],
value = X[,j])
}
V(net)$name = V(net)$vertex.names
# Chunk 5
summary(net)
# Chunk 6
E(net)
V(net)
list.vertex.attributes(net)
# Chunk 7
net = delete_vertices(net, which(igraph::degree(net) < 1))
# Chunk 8
D_net = igraph::degree(net)
sort(D_net,decreasing = T)[1:5]
# Chunk 9
par(mfrow=c(1,2))
hist(D_net,nclass = 100, main = "Degree distribution", xlab = "Degree")
hist(igraph::degree(net,normalized = T),nclass=100, main = "Degree distribution (normalized)", xlab = "Normalized degree")
# Chunk 10
edge_density(graph = net)
# Chunk 11
shortest_paths(graph = net,from = "aiden7757",to = "_nickluna_")$vpath
# Chunk 12
average.path.length(graph = net)
# Chunk 13
par(mfrow = c(1,1))
S = distances(graph = net)
image(S,col = gray.colors(n = diameter(net)))
# Chunk 14
C_net = igraph::closeness(graph = net)
B_net = igraph::betweenness(graph = net)
# Chunk 15
subg = components(graph = net)
subg$csize
# Chunk 16
net = igraph::delete.vertices(net, which(subg$membership != 1))
C_net = igraph::closeness(graph = net,normalized = T)
B_net = igraph::betweenness(graph = net,normalized = T)
D_net = igraph::degree(net,normalized = T)
# Chunk 17
sort(C_net,decreasing = T)[1:5]
sort(D_net,decreasing = T)[1:5]
sort(B_net,decreasing = T)[1:5]
# Chunk 18
par(mfrow=c(1,2))
hist(C_net, breaks = 50)
hist(B_net, breaks = 50)
# Chunk 19
sum(B_net > .1)
# Chunk 20
cor(cbind(B_net,D_net,C_net))
# Chunk 21
library(GGally)
ggpairs((data.frame(B_net,D_net,C_net))) + theme_bw()
# Chunk 22
aType = vertex_attr(graph = net,name="accounttype")
# Chunk 23
require(stringr)
aType = str_to_title(aType)
aType = str_replace(aType,"Ukranian","Russian")
aType = str_replace(aType,"Commercial|Local","News")
aType = str_replace(aType,"Koch","Fear")
aType = str_replace(aType,"^$|Arabic|\\?", "Other")
table(aType)
# Chunk 24
require(tidyverse)
df_pl = data.frame(den=D_net, clos = C_net, bet = B_net, x=aType)
#gather(df_pl,"den","clos","bet")
df_pl %>% dplyr::filter(x %in% c("Right","Left","Russian")) %>%
gather(.,"den","clos","bet",key="stat",value = "value") %>%
ggplot(aes(value,after_stat(density),color=x)) +
geom_freqpoly(lwd=1)+
facet_wrap(~stat,scales="free")+
theme_bw()
table(membership(gr))
assortativity(net,membership(gr))
modularity(net,factor(aType))
assortativity(net,factor(aType))
# Chunk 1: setup
knitr::opts_chunk$set(echo = T, fig.align = "center", out.width = '80%', warning = F, message = F)
# Chunk 19
DB <- read.csv("Datasets/HappinessAlcoholConsumption.csv")
DB <- read.csv("Datasets/HappinessAlcoholConsumption.csv")
DB <- read.csv("Lab01/Datasets/HappinessAlcoholConsumption.csv")
is.data.frame(DB)
str(DB)
head(DB)
View(DB)
# Variable "Country"
DB$Country <- as.factor(DB$Country)
levels(DB$Country)
levels(DB$Region)
# Variable "Region"
DB$Region <- as.factor(DB$Region)
levels(DB$Region)
# Variable "Hemisphere"
table(DB$Hemisphere)
DB$Hemisphere <- ifelse(DB$Hemisphere == "noth", "north", DB$Hemisphere)
table(DB$Hemisphere)
DB$Hemisphere <- as.factor(DB$Hemisphere)
levels(DB$Hemisphere)
var_interest_numeric <- colnames(DB)[!(colnames(DB) %in% c("Country", "Region", "Hemisphere"))]
for(i in c(1:length(var_interest_numeric))){
hist(DB[,var_interest_numeric[i]], main = var_interest_numeric[i], xlab = var_interest_numeric[i])
}
var_interest_factor <- c("Region", "Hemisphere")
for(i in c(1:length(var_interest_factor))){
barplot(table(DB[,var_interest_factor[i]]), main = var_interest_factor[i], xlab = var_interest_factor[i])
}
DB <- read.csv("../Lab01/Datasets/HappinessAlcoholConsumption.csv")
DB <- read.csv("../Lab01/Datasets/HappinessAlcoholConsumption.csv")
getwd()
DB <- read.csv("Lab01/Datasets/HappinessAlcoholConsumption.csv")
# Chunk 1: setup
knitr::opts_chunk$set(echo = T, fig.align = "center", out.width = '80%', warning = F, message = F)
# Chunk 1: setup
knitr::opts_chunk$set(echo = T, fig.align = "center", out.width = '80%', warning = F, message = F)
DB <- read.csv("Datasets/HappinessAlcoholConsumption.csv")
DB <- read.csv("../Lab01/Datasets/HappinessAlcoholConsumption.csv")
# Chunk 1: setup
knitr::opts_chunk$set(echo = T, fig.align = "center", out.width = '80%', warning = F, message = F)
# Chunk 3
library(ggplot2)
# Chunk 5
(4+5)*3
# Chunk 9
TRUE==1
# Chunk 10
FALSE==0
# Chunk 13
ls()
# Chunk 16
pluto <- c(1:100) #create a vector containing number from 1 to 100
pluto1 <- seq(1,500,5) #create a vector with values from 1 to 500 for every 5 values
pluto2 <- rep(10,5) #create a vector where the value 10 is repeated 5 times
pluto[3] #recall the element of the vector pluto that is at position 3
pluto[1:3] #recall the element of the vector pluto that is in the 1,2,3 position
is.vector(pluto) #check that pluto is a vector
length(pluto) #dimension of vector pluto?
sort(pluto) #we sort pluto
pluto/5 #we divide each element of pluto by 5
# Chunk 17
foo <- matrix(data = 10, nrow = 10, ncol = 5) #Create a matrix
#with 10 rows and 5 columns containing only 10
foo1 <- matrix(data = foo, nrow = 20, ncol = 5, byrow = T) #let's put foo in
#foo! starting with rows (byrow)
foo2 <- cbind(pluto,pluto1) #Unit the two vectors by column
foo3 <- rbind(pluto,pluto1) #Unit the two vectors by row
dim(foo) #Dimension of foo?
ncol(foo) #number of columns of foo?
nrow(foo) #number of rows of foo?
is.matrix(foo) #Check that foo is an array
# Chunk 18
rick <- data.frame(foo2[1:20,]) #create a dataframe named rick
#which takes the first 20 observations of foo2
dim(rick) #dimension? nrow, ncol etc as a matrix
colnames(rick) #column names?
rownames(rick) #row names?
rownames(rick) <- letters[1:dim(rick)[1]] #rename the rows of rick
#as the first 20 letters of the alphabet
rick[10,2] #recall the element of the 10th row and 2nd column
rick$pluto #recall the variable pluto (first column)
# Chunk 20
DB <- read.csv("Datasets/HappinessAlcoholConsumption.csv")
# Chunk 22: dat_structure
# Variable "Country"
DB$Country <- as.factor(DB$Country)
levels(DB$Country)
# Variable "Region"
DB$Region <- as.factor(DB$Region)
levels(DB$Region)
# Variable "Hemisphere"
table(DB$Hemisphere)
DB$Hemisphere <- ifelse(DB$Hemisphere == "noth", "north", DB$Hemisphere)
DB$Hemisphere <- as.factor(DB$Hemisphere)
levels(DB$Hemisphere)
# Chunk 23
summary(DB)
# Chunk 24: data_summary
out <- data.frame(
min = min(DB$HappinessScore),
quantile025 = quantile(DB$HappinessScore, 0.25),
median = median(DB$HappinessScore),
mean = mean(DB$HappinessScore),
quantile075 = quantile(DB$HappinessScore, 0.75),
max = max(DB$HappinessScore))
rownames(out) <- NULL #remove rownames of the objects out
out
# Chunk 25: histograms0
var_interest_numeric <- colnames(DB)[!(colnames(DB) %in% c("Country", "Region", "Hemisphere"))]
for(i in c(1:length(var_interest_numeric))){
hist(DB[,var_interest_numeric[i]], main = var_interest_numeric[i], xlab = var_interest_numeric[i])
}
# Chunk 26
library(dplyr)
library(ggplot2)
library(tidyr)
# Chunk 27
DB %>%
dplyr::select(all_of(var_interest_numeric)) %>%
gather(cols, value) %>%
ggplot(aes(x = value)) + geom_histogram(bins = 50) + facet_wrap(.~ cols, ncol = 3, scales = "free")
# Chunk 28: histograms1
var_interest_factor <- c("Region", "Hemisphere")
for(i in c(1:length(var_interest_factor))){
barplot(table(DB[,var_interest_factor[i]]), main = var_interest_factor[i], xlab = var_interest_factor[i])
}
# Chunk 29
DB %>%
dplyr::select(all_of(var_interest_factor)) %>%
gather(cols, value) %>%
ggplot(aes(x = value)) + geom_bar() + facet_wrap(.~ cols, ncol = 3, scales = "free")
# Chunk 30: histograms2
for(i in c(1:length(var_interest_numeric))){
boxplot(DB[,var_interest_numeric[i]], main = var_interest_numeric[i], xlab = var_interest_numeric[i])
}
# Chunk 31
DB %>%
dplyr::select(all_of(var_interest_numeric)) %>%
gather(cols, value) %>%
ggplot(aes(x = value)) + geom_boxplot() + facet_wrap(.~ cols, ncol = 3, scales = "free")
# Chunk 32: correlation1
# Correlation matrix
corr_matrix <- round(cor(DB[,var_interest_numeric]), 2)
corr_matrix[lower.tri(corr_matrix)] <- 0
corr_matrix
# Chunk 33: correlation2
# Correlogram
library(ggcorrplot)
ggcorrplot(corr_matrix,
type = "upper",
lab = T,
lab_size = 7,
outline.col = "white",
colors = c("tomato2",
"white",
"springgreen3"),
title = "",
ggtheme = theme_gray,
p.mat = cor_pmat(corr_matrix),
pch.cex = 30,
tl.cex = 20)
# Chunk 34: correlation3
heatmap(corr_matrix)
# Chunk 35: correlation4
corrplot::corrplot(corr_matrix)
# Chunk 36: scatterplot_matrices
# We plot a matrix of scatterplot using the "splom" function from the "lattice" library
lattice::splom(x = DB[, var_interest_numeric], pch = "*")
# We can imitate the latter plot using also basic R:
pairs(DB[, var_interest_numeric], pch = "*", col = "#0080ff")
# Chunk 37: train_test
# Number of total rows in the data set (= maximum sample size from which we can draw random units):
n <- nrow(DB)
# Size of the training data set (3/4 of "census_tracts"):
size <- round(0.75 * n)
# We set a seed for reproducibility of our results:
set.seed(666)
# We sample the row indeces representing the statistical units that will end up in the training set:
row.ind <- sample(x = 1:n, size = size)
train_db <- DB[row.ind,]
# The remaining row indeces corresponds to the statistical units that end up in the test set:
test_db <- DB[-row.ind,]
# Check on factors (we must have all levels of categorical variables in both training and test sets):
all.equal(levels(train_db$Country), levels(test_db$Country))
all.equal(levels(train_db$Region), levels(test_db$Region))
# Chunk 38: slr1
# Simple linear regression model:
lm.fit.simple <- lm(formula = HappinessScore ~ Beer_PerCapita,
# formula equivalent to the model in "EQ. 1"
data = DB)
summary(lm.fit.simple)
# Chunk 39: slr2b
# Confidence intervals
confint(lm.fit.simple)
# Chunk 40: slr3
# Confidence intervals
predict(lm.fit.simple,                                       # lm fitted object
newdata = data.frame(Beer_PerCapita = (c(40, 125, 200))),          # new values for Beer_PerCapita (to be given as data.frame)
interval = "confidence")                             # type of interval
# Prediction intervals
predict(lm.fit.simple,                                       # lm fitted object
newdata = data.frame(Beer_PerCapita = (c(40, 125, 200))),          # new values for Beer_PerCapita (to be given as data.frame)
interval = "prediction")                             # type of interval
# Chunk 41: slr4
# New data
new_Beer_PerCapita <- seq(min(DB$Beer_PerCapita), max(DB$Beer_PerCapita), by = 0.05)
# 95% confidence interval for regression line
conf_interval <- predict(lm.fit.simple,
newdata = data.frame(Beer_PerCapita = new_Beer_PerCapita),
interval = "confidence", level = 0.95)
# 95% prediction interval for regression line
pred_interval <- predict(lm.fit.simple,
newdata = data.frame(Beer_PerCapita = new_Beer_PerCapita),
interval = "prediction", level = 0.95)
# Plot
plot(x = DB$Beer_PerCapita,                                         # `Beer_PerCapita` on x-axis
y = DB$HappinessScore,                                          # `HappinessScore` on y-axis
pch = "*", cex = 0.8,                                            # graphical parameter
xlab = "Beer Per Capita",                  # labels of x-axis and y-axis
ylab = "mHappiness Score")
abline(lm.fit.simple, col = "blue")                          # regression line
lines(new_Beer_PerCapita, conf_interval[,2], col = "blue", lty = 2)            # lower extreme conf. interval
lines(new_Beer_PerCapita, conf_interval[,3], col = "blue", lty = 2)            # upper extreme conf. interval
lines(new_Beer_PerCapita, pred_interval[,2], col = "red", lty = 2)             # lower extreme pred. interval
lines(new_Beer_PerCapita, pred_interval[,3], col = "red", lty = 2)             # upper extreme pred. interval
legend("topright",                                                    # legend
title = expression(1-alpha == 0.95 ~ ("95%")),
legend = c("Conf. interval", "Pred. interval"),
lty = 2, col = c("blue", "red"))
# Chunk 42: slr5
# Diagnostic plots
par(mfrow = c(2,2))
plot(lm.fit.simple)
par(mfrow = c(1,1))
# # Alternatively
# autoplot(lm.fit.simple, label.size = 3)
# Chunk 43: mlr1
lm.fit.multiple <- lm(HappinessScore ~. -Country, data = DB)
summary(lm.fit.multiple)
# car::vif(lm.fit.multiple)
# Chunk 44: it
lm.fit.interactions <- lm(HappinessScore ~ Beer_PerCapita*Hemisphere, data = DB)
summary(lm.fit.interactions)
# Chunk 45: nonLinearVar1
lm.fit.nonlinear <- lm(HappinessScore ~ Beer_PerCapita + I(Beer_PerCapita^2), data = DB)
summary(lm.fit.nonlinear)
anova(lm.fit.simple, lm.fit.nonlinear)
par(mfrow = c(2,2))
plot(lm.fit.nonlinear)
par(mfrow = c(1,1))
# # Alternatively
# autoplot(lm.fit.nonlinear, label.size = 3)
plot(lm.fit.nonlinear)
load("Lab02/lab02.RData")
table(aType)
war = read.table('war.txt')
war = read.table('Additional_material/war.txt')
war = as.matrix(war)
str(war)
war <  0
1*(war <  0)
war
isSymmetric(war)
lattice::levelplot(war)
library(lattice)
war
war = as.matrix(war)
str(war)
war = 1*(war <  0)
isSymmetric(war)
lattice::levelplot(war)
library(igraph)
net = graph_from_adjacency_matrix(war, mode = 'undirected', weighted = NULL, diag=F)
str(net)
object.size(war)
war
net
object.size(war)
object.size(net)
as.numeric(object.size(war) / object.size(net))
E(net)
V(net)
edge_density(net)
0.05*1000
0.05*100
zeros = which(degree(net) < 1)
length(zeros)
#?shortest_paths
shortest_paths(net, from = 'ITA')
V(net)[50]
#altre misure viste a lezione
hist(sqrt(betweenness(net)), breaks = 20, col = 'lavender')
diameter(net)
plot(net)
set.seed(1)
net_lo = layout_with_graphopt(net, niter =3500)
plot(net,layout= net_lo)
### Improve rempoving disconnetted
rem = c('HAI', 'DOM', 'ARG', 'CHL')
net = delete.vertices(net, rem)
set.seed(1)
net_lo = layout_with_graphopt(net, niter =3500)
###Again
V(net)$size = degree(net)+1
plot(net, layout= net_lo)
V(net)$size = sqrt(betweenness(net))
plot(net, layout= net_lo)
length(V(net))
comm = multilevel.community(net)#This function implements the multi-level modularity optimization algorithm for finding community structure
comm
comm2 = cluster_fast_greedy(net)
gr = membership(comm)
gr2 = membership(comm2)
modularity(net,gr)
modularity(net,gr2)
assortativity(net,gr)
assortativity(net,gr2)
##numbers of clusters
max(gr2)
set.seed(1)
net_col = as.vector(gr2)
net_col
V(net)$color = net_col
plot(net, layout= net_lo)
V(net)$label.color = 'white'
V(net)$label.font = 2
V(net)$size = pmin(sqrt(betweenness(net)), 10)+2
par(bg='black')
plot(net, layout= net_lo)
rem
rem_id = which(colnames(war) %in% rem)
rem_id
war_red = war[-c(rem_id, empty), -c(rem_id, empty)]
war_red = war[-c(rem_id, zeros), -c(rem_id, zeros)]
net2 = as.network(war_red, directed=F)
library(ergm)
net2 = as.network(war_red, directed=F)
par(bg='white')
plot(net2)
summary(net2)
m0 = ergm(net2~edges, estimate = 'MPLE')
summary(m0)
##How to interpret coefficients:
exp(m0$coef)/(1+exp(m0$coef))
plogis(m0$coef)
summary(net2)
m1 = ergm(net2~edges+triangles, estimate = 'MPLE')
summary(m1)
ergmMPLE(net2~edges+triangles)
#We begin with a simple model, containing only one term that represents the total number of edges in the network
summary(net2 ~ edges)
#The corresponding probability is obtained by taking the expit,
#or inverse logit, of our estimated parameter:
exp(m0$coef)/(1+exp(m0$coef))
plogis(m0$coef)
#We begin with a simple model, containing only one term that represents the total number of edges in the network
summary(net2 ~ edges)
0.06594071*109
0.06594071/109
109/0.06594071
net2
net2 ~ edges
edges
#We begin with a simple model, containing only one term that represents the total number of edges in the network
summary(net2 ~ edges)
triangles
##### interpretation
### if an arc carries no triangle
plogis(coef(m1)[1])
## If a node carries a triangle
plogis(c(1,1) %*% coef(m1))
## If a node carries two triangles
plogis(c(1,2) %*% coef(m1))
c(1,2) %*% coef(m1)
coef(m1)
plogis(coef(m1)[1,2])
coef(m1)
plogis(coef(m1)[2])
## If a node carries two triangles
plogis(c(1,2) %*% coef(m1))
c(1,2)
coef(m1)
exp(m1$coef)/(1+exp(m1$coef))
## If a node carries two triangles
plogis(c(1,2) %*% coef(m1))
## If a node carries two triangles
plogis(c(1,2) %*% coef(m1))
## If a node carries a triangle
plogis(c(1,1) %*% coef(m1))
##### interpretation
### if an arc carries no triangle
plogis(coef(m1)[1])
plogis(m0$coef)
#The corresponding probability is obtained by the inverse logit,
#of our estimated parameter:
exp(m0$coef)/(1+exp(m0$coef))
###GOF
gof_m1 = gof(m1)#Conduct Goodness-of-Fit Diagnostics on a Exponential Family Random Graph Model
gof_m1
plot(gof(m1~degree))
plot(gof(m1~distance))
plot(m1)
plot(gof(m1~degree))
plot(gof(m1~triangle))
plot(gof(m1~triangles))
plot(gof(m1~triadcensus))
plot(gof(m1~distance))
#We begin with a simple model, containing only one term that
#represents the total number of edges in the network
summary(net2~edges) # how many edges?
m2 <- ergm(net2~edges+nodecov('age.month'))
net2$mel
net2$oel
V(net2)
V(net)
rand <- c(rep(0, 30),rep(1, 30))
m2 <- ergm(net2~edges+nodecov('rand'))
rand <- rnorm(62)
m2 <- ergm(net2~edges+nodecov('rand'))
